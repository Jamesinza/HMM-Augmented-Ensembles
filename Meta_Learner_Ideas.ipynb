{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6ceda-2f49-4f9c-9d47-d65c75982e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# === Part 1: Second Meta-Learner Blueprints ===\n",
    "\n",
    "def build_gnn_meta_learner(num_models, num_classes, hidden_dim=128, num_layers=2):\n",
    "    \"\"\"\n",
    "    Graph-based meta learner: treats each base model output as a node,\n",
    "    learns interactions via message passing.\n",
    "    Input shape: (batch, num_models, num_classes)\n",
    "    Output: (batch, num_classes)\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(num_models, num_classes), name=\"gnn_inputs\")\n",
    "    x = inputs\n",
    "    for i in range(num_layers):\n",
    "        # node-wise transform\n",
    "        node_feats = layers.Dense(hidden_dim, activation='relu')(x)\n",
    "        # compute adjacency (learned) via similarity\n",
    "        # Here: simple self-attention as adjacency\n",
    "        attn_scores = tf.matmul(node_feats, node_feats, transpose_b=True)\n",
    "        attn_weights = tf.nn.softmax(attn_scores, axis=-1)\n",
    "        # message passing\n",
    "        x = tf.matmul(attn_weights, node_feats)\n",
    "    # readout: mean over nodes\n",
    "    readout = tf.reduce_mean(x, axis=1)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name=\"gnn_meta_output\")(readout)\n",
    "    return Model(inputs, outputs, name=\"GNN_MetaLearner\")\n",
    "\n",
    "\n",
    "def build_tabnet_meta_learner(num_models, num_classes, feature_dim=128, num_steps=3, relaxation=1.5):\n",
    "    \"\"\"\n",
    "    Simplified TabNet: sequential feature masking + shared feature transformer.\n",
    "    Input flattened: (batch, num_models * num_classes)\n",
    "    \"\"\"\n",
    "    from tensorflow_addons.layers import FeatureTransformer  # hypothetical\n",
    "    inputs = layers.Input(shape=(num_models * num_classes,), name=\"tabnet_inputs\")\n",
    "    masks = []\n",
    "    aggregated = 0\n",
    "    shared_block = layers.Dense(feature_dim, activation='relu')\n",
    "    for step in range(num_steps):\n",
    "        # compute mask\n",
    "        mask = layers.Dense(num_models * num_classes, activation='softmax', name=f\"mask_{step}\")(inputs)\n",
    "        # relax mask for exploration\n",
    "        mask = tf.pow(mask, 1.0 / relaxation)\n",
    "        masks.append(mask)\n",
    "        # apply mask\n",
    "        masked_x = layers.Multiply()([inputs, mask])\n",
    "        # feature transformer\n",
    "        transformed = shared_block(masked_x)\n",
    "        aggregated = aggregated + transformed\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name=\"tabnet_meta_output\")(aggregated)\n",
    "    return Model(inputs, outputs, name=\"TabNet_MetaLearner\")\n",
    "\n",
    "\n",
    "def build_moe_meta_learner(num_models, num_classes, expert_units=128, num_experts=4, k=2):\n",
    "    \"\"\"\n",
    "    Mixture of Experts meta learner with k-sparse gating.\n",
    "    Input: (batch, num_models * num_classes)\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(num_models * num_classes,), name=\"moe_inputs\")\n",
    "    # gating network\n",
    "    gate_logits = layers.Dense(num_experts, name=\"gate_logits\")(inputs)\n",
    "    # k-sparse selection\n",
    "    top_k = tf.math.top_k(gate_logits, k=k, sorted=False)\n",
    "    mask = tf.reduce_sum(tf.one_hot(top_k.indices, depth=num_experts), axis=1)\n",
    "    gate_weights = tf.nn.softmax(gate_logits) * mask\n",
    "    gate_weights = gate_weights / tf.reduce_sum(gate_weights, axis=-1, keepdims=True)\n",
    "    # experts\n",
    "    expert_outputs = []\n",
    "    for i in range(num_experts):\n",
    "        expert = layers.Dense(expert_units, activation='relu', name=f\"expert_{i}\")(inputs)\n",
    "        expert = layers.Dense(num_classes, name=f\"expert_out_{i}\")(expert)\n",
    "        expert_outputs.append(expert)\n",
    "    stack = tf.stack(expert_outputs, axis=-1)  # shape: (batch, num_classes, num_experts)\n",
    "    weighted = tf.matmul(stack, tf.expand_dims(gate_weights, -1))  # (batch, num_classes, 1)\n",
    "    outputs = tf.squeeze(weighted, axis=-1, name=\"moe_meta_output\")\n",
    "    outputs = layers.Activation('softmax')(outputs)\n",
    "    return Model(inputs, outputs, name=\"MoE_MetaLearner\")\n",
    "\n",
    "\n",
    "# === Part 2: Meta-Meta Learner with Gumbel-Softmax Routing ===\n",
    "\n",
    "def build_meta_meta_learner(meta_models, num_classes, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Combines multiple meta learners via Gumbel-Softmax routing.\n",
    "    meta_models: list of Keras models that output (batch, num_classes)\n",
    "    \"\"\"\n",
    "    # aggregate inputs: assume all meta_models share same input\n",
    "    meta_inputs = layers.Input(shape=meta_models[0].input_shape[1:], name=\"meta_meta_input\")\n",
    "    # get meta outputs\n",
    "    meta_outputs = [m(meta_inputs) for m in meta_models]\n",
    "    # stack outputs: shape (batch, num_classes, n_meta)\n",
    "    stack = tf.stack(meta_outputs, axis=-1)\n",
    "    # routing logits\n",
    "    routing_logits = layers.Dense(len(meta_models), name=\"routing_logits\")(layers.Flatten()(meta_inputs))\n",
    "    # Gumbel-Softmax sampling\n",
    "    gumbel = tfp.distributions.RelaxedOneHotCategorical(temperature, logits=routing_logits)\n",
    "    weights = gumbel.sample()\n",
    "    # apply weights: (batch, num_meta)\n",
    "    weights_exp = tf.expand_dims(weights, axis=1)  # (batch,1,n_meta)\n",
    "    mixed = tf.matmul(stack, weights_exp)  # (batch,num_classes,1)\n",
    "    final_out = tf.squeeze(mixed, axis=-1)\n",
    "    final_out = layers.Activation('softmax', name=\"final_output\")(final_out)\n",
    "    return Model(meta_inputs, final_out, name=\"MetaMetaLearner\")\n",
    "\n",
    "\n",
    "# === Part 3: Training Loop with Dynamic Meta-Learner Weighting ===\n",
    "\n",
    "def train_meta_stack(base_data, labels, base_models, meta_models, meta_meta_model,\n",
    "                     epochs=10, batch_size=32, lr=1e-3):\n",
    "    \"\"\"\n",
    "    base_data: raw inputs to feed base models\n",
    "    meta_models: [transformer_meta, second_meta]\n",
    "    meta_meta_model: final arbiter\n",
    "    \"\"\"\n",
    "    opt = tf.keras.optimizers.Adam(lr)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    # metrics\n",
    "    train_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for step, (x_batch, y_batch) in enumerate(tf.data.Dataset.from_tensor_slices((base_data, labels)).batch(batch_size)):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # gather base predictions\n",
    "                base_preds = [m(x_batch, training=False) for m in base_models]\n",
    "                # stack base preds for meta input\n",
    "                meta_input = tf.concat(base_preds, axis=-1)\n",
    "                # meta outputs\n",
    "                meta_outs = [mm(meta_input, training=True) for mm in meta_models]\n",
    "                # meta-meta output\n",
    "                final_pred = meta_meta_model(meta_input, training=True)\n",
    "                # compute loss\n",
    "                loss = loss_fn(y_batch, final_pred)\n",
    "            # compute grads only for meta and meta-meta\n",
    "            trainable_vars = []\n",
    "            for mm in meta_models + [meta_meta_model]:\n",
    "                trainable_vars += mm.trainable_variables\n",
    "            grads = tape.gradient(loss, trainable_vars)\n",
    "            opt.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "            # update metrics\n",
    "            train_acc.update_state(y_batch, final_pred)\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Step {step}: loss = {loss.numpy():.4f}, acc = {train_acc.result().numpy():.4f}\")\n",
    "        print(f\"Epoch {epoch+1} Accuracy: {train_acc.result().numpy():.4f}\")\n",
    "        train_acc.reset_states()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
