{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3abc3ac-32b4-4083-9f60-e031e3a3ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:54:04.015278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748984044.035498   18429 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748984044.041671   18429 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748984044.057122   18429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748984044.057148   18429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748984044.057150   18429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748984044.057152   18429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-03 22:54:04.062248: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x76314a9ad520>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/james/miniconda3/envs/py312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building dataframe using real data...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_model(model, name):\n",
    "    pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
    "    true = y_test\n",
    "    acc = accuracy_score(true, pred)\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "# --- Meta-Meta Learner ---\n",
    "def create_meta_meta_learner(meta_learners, seed=42):\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"MetaMeta_input\")\n",
    "    meta_outputs = [ml(inputs) for ml in meta_learners]\n",
    "    stacked = layers.Concatenate()(meta_outputs)\n",
    "    stacked = layers.RepeatVector(2)(stacked)\n",
    "    x = layers.LSTM(64, activation='tanh', seed=seed)(concat)\n",
    "    out = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=out, name=\"MetaMetaLearner\")\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Meta Learner ---\n",
    "def create_meta_learner(base_models, name=\"MetaLearner\", seed=42):\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=f\"{name}_input\")\n",
    "    base_outputs = [model(inputs) for model in base_models]\n",
    "    stacked = layers.Concatenate()(base_outputs)\n",
    "    stacked = layers.RepeatVector(3)(stacked)\n",
    "    x = layers.LSTM(64, activation='tanh', seed=seed)(stacked)\n",
    "    out = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    meta_model = models.Model(inputs=inputs, outputs=out, name=name)\n",
    "    meta_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return meta_model\n",
    "\n",
    "# --- Base Model Generator ---\n",
    "def create_base_model(hidden_units=32, seed=42):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.LSTM(hidden_units, activation='tanh', seed=seed),\n",
    "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adamw', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Train Base Models ---\n",
    "def train_base_models(num_models=3, label=\"A\"):\n",
    "    models = []\n",
    "    for i in range(num_models):\n",
    "        callback = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
    "        ]\n",
    "        model = create_base_model(hidden_units=32 + i * 16, seed=i)\n",
    "        print(f\"Training BaseModel_{label}{chr(97+i)}...\")\n",
    "        model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val),\n",
    "                  verbose=1, callbacks=callback, class_weight=class_weights_dict)\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "# --- Get real world data ---\n",
    "def get_real_data(dataset):\n",
    "    \"\"\"Load and preprocess real data from a CSV.\"\"\"\n",
    "    print('\\nBuilding dataframe using real data...')\n",
    "\n",
    "    if dataset == 'Euro' or dataset == 'Thunderball' or dataset == 'Take5':\n",
    "        cols = ['A', 'B', 'C', 'D', 'E']\n",
    "    else:\n",
    "        cols = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "\n",
    "    if dataset == 'Take5':\n",
    "        df = pd.read_csv(f'datasets/training/{dataset}_Full.csv').astype(np.int16)\n",
    "    else:\n",
    "        df = pd.read_csv(f'datasets/UK/{dataset}_ascend.csv').astype(np.int16)\n",
    "\n",
    "    df = df[cols].dropna().astype(np.int16)\n",
    "    # Format each element as 2-digit string and then flatten digits into an array.\n",
    "    df = df.map(lambda x: f'{x:02d}')\n",
    "    flattened = df.values.flatten()\n",
    "    full_data = np.array([int(d) for num in flattened for d in str(num)], dtype=np.int16)\n",
    "    return full_data\n",
    "\n",
    "# --- Generate data sequences ---\n",
    "def generate_dataset(data, wl=10, features=1):\n",
    "    X_test = np.empty([len(data)-wl, wl, features], dtype=np.int8)\n",
    "    y_test = np.empty([len(data)-wl, 1], dtype=np.int8)\n",
    "    for i in range(len(data)-wl):\n",
    "        X_test[i] = data[i:i+wl]\n",
    "        y_test[i] = data[i+wl]    \n",
    "    return X_test, y_test\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "NUM_CLASSES = 10\n",
    "NOISE_LEVEL = 0.1\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "WL = 10\n",
    "dataset = 'Take5'\n",
    "wl = 10\n",
    "\n",
    "X_raw = get_real_data(dataset)\n",
    "data = X_raw.reshape(-1,1)\n",
    "features = data.shape[1]\n",
    "input_shape = (wl, features)\n",
    "\n",
    "X, y = generate_dataset(data, wl, features)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
    "\n",
    "unique_classes = np.unique(y.flatten())\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=y.flatten())\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "base_models_1 = train_base_models(label=\"1\")\n",
    "base_models_2 = train_base_models(label=\"2\")\n",
    "\n",
    "# --- Train Meta Learners ---\n",
    "meta_learner_1 = create_meta_learner(base_models_1, name=\"Meta1\", seed=4)\n",
    "meta_learner_2 = create_meta_learner(base_models_2, name=\"Meta2\", seed=5)\n",
    "\n",
    "print(\"Training MetaLearner 1...\")\n",
    "callback = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
    "]\n",
    "meta_learner_1.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val),\n",
    "                   verbose=1, callbacks=callback, class_weight=class_weights_dict)\n",
    "\n",
    "print(\"Training MetaLearner 2...\")\n",
    "callback = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
    "]\n",
    "meta_learner_2.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val),\n",
    "                   verbose=1, callbacks=callback, class_weight=class_weights_dict)\n",
    "\n",
    "# --- Train Meta-Meta Learners ---\n",
    "meta_meta_learner = create_meta_meta_learner([meta_learner_1, meta_learner_2], seed=6)\n",
    "\n",
    "print(\"Training Meta-Meta Learner...\")\n",
    "callback = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
    "]\n",
    "meta_meta_learner.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val),\n",
    "                      verbose=1, callbacks=callback, class_weight=class_weights_dict)\n",
    "\n",
    "print(\"\\n--- Final Evaluation ---\")\n",
    "subs = ['a', 'b', 'c']\n",
    "for i, sub in enumerate(subs):\n",
    "    evaluate_model(base_models_1[i], f\"BaseModel_1{sub}\")\n",
    "    evaluate_model(base_models_2[i], f\"BaseModel_2{sub}\")\n",
    "evaluate_model(meta_learner_1, \"MetaLearner_1\")\n",
    "evaluate_model(meta_learner_2, \"MetaLearner_2\")\n",
    "evaluate_model(meta_meta_learner, \"MetaMetaLearner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66deb26e-e459-47ab-bae8-c204e16709eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
